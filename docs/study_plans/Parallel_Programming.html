<h3>Parallel programming</h3>
<ol>
<li>Basic concepts &amp; general stuff:</li>
<li>NHR@FAU - Parallel Programming 2020:<ul>
<li>[Lecture 2 - Computer Architecture (https://www.youtube.com/watch?v=T5NjmYeSC1s)</li>
<li><a href="https://www.youtube.com/watch?v=yB5Cue_4R8k">Lecture 3 - Parallelism</a></li>
</ul>
</li>
<li>Tom Nurkkala - Parallel and Distributed Computing:<ul>
<li><a href="https://www.youtube.com/watch?v=isRq60R4b9U">Flynn's Taxonomy and Metrics</a></li>
<li><a href="https://www.youtube.com/watch?v=ZIK5o7Rbl6s">C Pointer Refresher</a></li>
<li><a href="https://www.youtube.com/watch?v=tWX3M89IMek">Parallel Program Design 1</a></li>
<li><a href="https://www.youtube.com/watch?v=FUfXYvKi8Ks">Parallel Program Design 2</a></li>
</ul>
</li>
<li>
<p>Nptel Week 1:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=LkDgKBHjwz8">Cache and Temporal Locality</a></li>
<li><a href="">Cache, Memory bandwidth and Spatial Locality</a></li>
<li><a href="https://www.youtube.com/watch?v=TDW37ggXVms">Intuition for Shared and Distributed Memory architectures</a></li>
<li><a href="https://www.youtube.com/watch?v=3m9g-Bv1tkk">Shared and Distributed Memory architectures</a></li>
<li><a href="https://www.youtube.com/watch?v=BUFbnisqkAM">Interconnection networks in Distributed Memory architectures</a></li>
</ul>
</li>
<li>
<p>OpenMP:</p>
</li>
<li>NHR@FAU - Parallel Programming 2020:<ul>
<li><a href="https://www.youtube.com/watch?v=1Txkbx-AcR0">Lecture 4 - Basic OpenMP</a></li>
<li><a href="https://www.youtube.com/watch?v=OCNEZThY6Ic">Lecture 5 - More Basic OpenMP</a></li>
<li><a href="https://www.youtube.com/watch?v=o_MLhGkaGq4">Lecture 6 - Advanced OpenMP and performance issues</a></li>
<li><a href="https://www.youtube.com/watch?v=3vuano-RhEc">Lecture 7 - ccNUMA and wavefront parallelization with OpenMP</a></li>
</ul>
</li>
<li>NPTEL Week 2:<ul>
<li><a href="https://www.youtube.com/watch?v=Ka3rBhwMgXg">OpenMP: A parallel Hello World Program</a></li>
<li><a href="https://www.youtube.com/watch?v=PwvvN3s9aSw">Program with Single thread</a></li>
<li><a href="https://www.youtube.com/watch?v=UXYEGSlPqik">Program Memory with Multiple threads and Multi-tasking</a></li>
<li><a href="https://www.youtube.com/watch?v=1GvGHgyLYhc">Context Switching</a></li>
<li><a href="https://www.youtube.com/watch?v=wkJDYOjsGP4">OpenMP: Basic thread functions</a></li>
<li><a href="https://www.youtube.com/watch?v=tdpNIiAubY4">OpenMP: About OpenMP</a></li>
<li><a href="https://www.youtube.com/watch?v=Ua1p-FyplkI">Race Conditions</a></li>
<li><a href="https://www.youtube.com/watch?v=ukELuDFf41w">OpenMP: Scoping variables and some race conditions</a></li>
<li><a href="https://www.youtube.com/watch?v=GX1DpC37LeM">OpenMP: thread private variables and more constructs</a></li>
<li><a href="https://www.youtube.com/watch?v=OhL1Eazmr48">Shared Memory Consistency Models and the Sequential Consistency Model</a></li>
</ul>
</li>
<li>Nptel Week 3:<ul>
<li><a href="https://www.youtube.com/watch?v=oXjKhVMkq9g">Computing sum: first attempt at parallelization</a></li>
<li><a href="https://www.youtube.com/watch?v=IyBczHlcVjM">Manual distribution of work and critical sections</a></li>
<li><a href="https://www.youtube.com/watch?v=G6GkGHfjBas">Distributing for loops and reduction</a></li>
<li><a href="https://www.youtube.com/watch?v=KM1tW_PdScg">Vector-Vector operations (Dot product)</a></li>
<li><a href="https://www.youtube.com/watch?v=ghM_ssgwVBg">Matrix-Vector operations (Matrix-Vector Multiply)</a></li>
<li><a href="https://www.youtube.com/watch?v=K84-zInk_ec">Matrix-Matrix operations (Matrix-Matrix Multiply)</a></li>
<li><a href="https://www.youtube.com/watch?v=Nak1HHPuKbs">Introduction to tasks</a></li>
<li><a href="https://www.youtube.com/watch?v=qiA_dvI36i8">Task queues and task execution</a></li>
<li><a href="https://www.youtube.com/watch?v=hYTQwJXnbuo">Accessing variables in tasks</a></li>
<li><a href="https://www.youtube.com/watch?v=1nDgmsyKKQw">Completion of tasks and scoping variables in tasks</a></li>
<li><a href="https://www.youtube.com/watch?v=jP3dhVXSDh8">Recursive task spawning and pitfalls</a></li>
</ul>
</li>
<li>
<p>Nptel Week 4:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=mQyZ3yLk_RY">Understanding LU Factorization</a></li>
<li><a href="https://www.youtube.com/watch?v=E8aBJsC0bY8">Parallel LU Factorization</a></li>
<li><a href="https://www.youtube.com/watch?v=BjZPuSO8Lvc">Locks</a></li>
<li><a href="https://www.youtube.com/watch?v=tpgIREdWL2s">Advanced Task handling</a></li>
<li><a href="https://www.youtube.com/watch?v=eHfZk-qep0E">Matrix Multiplication using tasks</a></li>
<li><a href="https://www.youtube.com/watch?v=fwmSN4GC7tE">The OpenMP Shared Memory Consistency Model</a></li>
</ul>
</li>
<li>
<p>MPI:</p>
</li>
<li>NHR@FAU - Parallel Programming 2020<ul>
<li><a href="https://www.youtube.com/watch?v=SCIYgBIm6LM">Lecture 8 - Introduction to MPI</a></li>
<li><a href="https://www.youtube.com/watch?v=3LiIZCWf_5Q">Lecture 9 - Point-to-point communication with MPI</a></li>
<li><a href="https://www.youtube.com/watch?v=ryC-L3qF33k">Lecture 10 - Collective communication, distributed-memory architecture</a></li>
<li>[Lecture 11- MPI data types, virtual topologies, and performance pitfalls(https://www.youtube.com/watch?v=nNCDgAU76D0)</li>
<li><a href="https://www.youtube.com/watch?v=rImhPTL3YXQ">Lecture 12 - MPI Input/Output</a></li>
<li><a href="https://www.youtube.com/watch?v=OHijtcMGm9I">Lecture 13 - Hybrid Programming with MPI and OpenMP</a></li>
</ul>
</li>
<li>CoffeeBeforeArch - Practical Parallelism in C++:<ul>
<li><a href="https://www.youtube.com/watch?v=a0V8KpLu7EY">Practical Parallelism in C++: MPI Basics</a></li>
<li><a href="https://www.youtube.com/watch?v=YVZq25G4p_g">Practical Parallelism in C++: MPI Synchronization</a></li>
<li><a href="https://www.youtube.com/watch?v=NApk1276GO4">Practical Parallelism in C++: MPI Gaussian Elimination Naive</a></li>
<li><a href="https://www.youtube.com/watch?v=2LxhcCjCLkU">Practical Parallelism in C++: MPI Gaussian Elimination Cyclic Striped</a></li>
<li><a href="https://www.youtube.com/watch?v=HRe7gLr_80M">Practical Parallelism in C++: MPI Fundamentals</a></li>
</ul>
</li>
<li>Nptel Week 5:<ul>
<li><a href="https://www.youtube.com/watch?v=AXs5BCectDM">Introduction to MPI and basic calls</a></li>
<li><a href="https://www.youtube.com/watch?v=-OuN64o5t_8">MPI calls to send and receive data</a></li>
<li><a href="https://www.youtube.com/watch?v=hhmF8il6kFA">MPI calls for broadcasting data</a></li>
<li><a href="https://www.youtube.com/watch?v=Neml-gHdj4E">MPI non blocking calls</a></li>
<li><a href="https://www.youtube.com/watch?v=v8r46ZTmlT4">Application distributed histogram updation</a></li>
<li><a href="https://www.youtube.com/watch?v=aiXvcPD-FbM">MPI collectives and MPI broadcast</a></li>
<li><a href="https://www.youtube.com/watch?v=-IC4pBF4kUk">MPI gathering and scattering collectives</a></li>
<li><a href="https://www.youtube.com/watch?v=S6JuoX9ZWhE">MPI reduction and alltoall collectives</a></li>
<li><a href="https://www.youtube.com/watch?v=svttXP9Sgvc">Discussion on MPI collectives design</a></li>
</ul>
</li>
</ol>